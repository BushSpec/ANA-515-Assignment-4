---
title: "Assignment Module 4 AG"
author: "Ani Gesheva"
date: "5/1/2021"
output:
  word_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Description of project.

This is a sentiment analysis model - a model, which categorizes words based on their positive, negative or neutral sentiment, and on the magnitude of that sentiment. It's a classification type of analysis.

The article defines sentiment analysis as the process of extracting opinions that have different positive, negative or neutral polarities. Sentiment analysis can reveal the nature of opinion that is reflected in documents, websites, social media feed, comments.  

Hence the goal is to extract information about sentiment from the books.

The project will perform sentiment analysis using Inner Join. 

It will be using the libraries ‘janeaustenr’, ‘stringr’ and ‘tidytext’.

The janeaustenr package provides text data - the text of Jane Austen's books. Tidytext helps perform text analysis. I will convert the text of the books into a tidy format using the unnest_tokens() function.

## Step 1: Installed the tidytext package and loaded it. 

Tidytext package comprises of sentiment lexicons that are present in the dataset of ‘sentiments’.

install.packages("tidytext")

```{r}
library(tidytext)
sentiments
```

Will make use of three general purpose lexicons: AFINN, bings and loughran. 

These lexicons use unigrams - a type of n-gram models that consist of a sequence of 1 item, that is, a word collected from a given textual data. 

The AFINN lexicon model scores the words in a range from -5 to 5. The increase in negativity corresponds to negative sentiment whereas an increase in positivity corresponds to positive one. 

The bing lexicon model classifies the sentiment into a binary category of negative or positive. 

The loughran model performs analysis of shareholder’s reports (I'd love to use that one some time to perform sentiment analysis on annual reports/10-Ks or letters to investors). 

Will use the bing lexicons to extract the sentiments out of the data.

## Step 2: Retrieved the bing lexicons using the get_sentiments() function. 

```{r}
get_sentiments("bing")
```
The bing lexicon consists of 6,786 words and their respective classifiction into positive or negative.

## Step 3: Loaded the following libraries: ‘janeaustenr’, ‘stringr’. 

I loaded tidytext above in step 1. 

In this project the data set is in the form of an R library/a package - it's janeaustenr: Jane Austen's Complete Novels.

It contains the full text for Jane Austen's 6 completed novels "Sense and Sensibility", "Pride and Prejudice", "Mansfield Park", "Emma", "Northanger Abbey", and "Persuasion".

The code below imports Jane Austen library (text of her books), and imports stringr. 

Then it assigns a data frame (object) called tidy_data to the Jane Austen books text, groups it by book, and cleans the data. 

Imported library dplyr as ungroup wouldn't work w/o it.

The last line of code (the unnest_tokens() fucntion) converts the text of Jane Austen's books into a tidy format.

This step performs the tidy operation on the text such that each row contains a single word (cleaning/preparing data).

```{r}
library(janeaustenr)
library(stringr)
library(dplyr)

tidy_data <- austen_books() %>%
 group_by(book) %>%
 mutate(linenumber = row_number(),
   chapter = cumsum(str_detect(text, regex("^chapter [\\divxlc]", 
                          ignore_case = TRUE)))) %>%
ungroup() %>%
unnest_tokens(word, text)
```

## Step 4: Used the bing lexicon and filter() over the words that correspond to joy/positive. 

Used the book "Emma" and derived its words to implement the sentiment analysis model. (Note the original article cites "Sense and Sensibility" - but it's "Emma" in the code below).

```{r}
positive_senti <- get_sentiments("bing") %>%
 filter(sentiment == "positive")
tidy_data %>%
 filter(book == "Emma") %>%
 semi_join(positive_senti) %>%
 count(word, sort = TRUE)
```

There are 668 positive works in "Emma". The most common is "well" (n=401), followed by "good" (n=359), "great" (n=264), "like"(n=200), and "better" (n=173). 

## Step 5: Calculated sentiment.

Used the spread() function to segregate the data into separate columns of positive and negative sentiments. Then used the mutate() function to calculate the total sentiment - the difference between positive and negative sentiment.

```{r}
library(tidyr)
bing <- get_sentiments("bing")
Emma_sentiment <- tidy_data %>%
 inner_join(bing) %>%
 count(book = "Emma" , index = linenumber %/% 80, sentiment) %>%
 spread(sentiment, n, fill = 0) %>%
 mutate(sentiment = positive - negative)
```

## Step 6: Visualized the words in the book “Emma” based on their corresponding positive and negative scores.

```{r}
library(ggplot2)
ggplot(Emma_sentiment, aes(index, sentiment, fill = book)) +
 geom_bar(stat = "identity", show.legend = TRUE) +
 facet_wrap(~book, ncol = 2, scales = "free_x")
```

Based on the chart above it seems that "Emma" is a predominantly positive book.

## Step 7: Counted the most common positive and negative words present in the novel using Inner Join.

```{r}
counting_words <- tidy_data %>%
 inner_join(bing) %>%
 count(word, sentiment, sort = TRUE)
head(counting_words)
```

Among the 2,585 entries (words), the most common are "miss", which is a negative word (n=1,855), "well" (positive, n=1,523), "good" (positive, n=1,380), "great" (positive, n=981) and "like" (positive, n=725). This chart doesn't entirely synch with above as we have the negative word "miss" as the most common.

## Step 8: Visualized the sentiment score. 

Plot the scores along the axis that is labeled with both positive as well as negative words using the ggplot() function to visualize the data based on their scores.

```{r}
counting_words %>%
 filter(n > 150) %>%
 mutate(n = ifelse(sentiment == "negative", -n, n)) %>%
 mutate(word = reorder(word, n)) %>%
 ggplot(aes(word, n, fill = sentiment))+
 geom_col() +
 coord_flip() +
 labs(y = "Sentiment Score")
```

## Step 9: Visualized with word cloud of the most recurring positive and negative words. 

Used the comparision.cloud() function to plot both negative and positive words in a single word cloud.

install.packages("reshape2")
install.packages("wordcloud")

```{r}
library(reshape2)
library(wordcloud)
tidy_data %>%
 inner_join(bing) %>%
 count(word, sentiment, sort = TRUE) %>%
 acast(word ~ sentiment, value.var = "n", fill = 0) %>%
 comparison.cloud(colors = c("red", "dark green"),
          max.words = 100)
```

## Conclusion.

The last two visualizations confirm the previous points - that while the text and sentiment is mostly positive, there is one negative word ("miss") that is most common and more common then the highest-frequency positive word ("well"). 
